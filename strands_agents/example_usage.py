#!/usr/bin/env python3
"""
Strands Agentå­—å¹•ç¿»è¯‘ç³»ç»Ÿä½¿ç”¨ç¤ºä¾‹
å±•ç¤ºå®Œæ•´çš„ç¿»è¯‘å·¥ä½œæµç¨‹
"""
import json
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from strands_agents.subtitle_translation_agent import create_subtitle_translation_agent
from strands_agents.enhanced_tools import (
    parse_srt_file,
    analyze_story_context,
    translate_with_context,
    validate_translation_quality,
    export_translated_srt
)

def example_1_basic_translation():
    """ç¤ºä¾‹1: åŸºç¡€ç¿»è¯‘æµç¨‹"""
    print("ğŸ¬ ç¤ºä¾‹1: åŸºç¡€å­—å¹•ç¿»è¯‘")
    print("=" * 50)
    
    # ç¤ºä¾‹SRTå†…å®¹
    srt_content = """1
00:00:01,000 --> 00:00:03,000
å¸ä»¤: å‚è°‹é•¿ï¼Œä»Šå¤©çš„è®­ç»ƒè®¡åˆ’å¦‚ä½•ï¼Ÿ

2
00:00:04,000 --> 00:00:06,000
å‚è°‹é•¿: æŠ¥å‘Šå¸ä»¤ï¼Œæ‰€æœ‰é˜Ÿå‘˜å·²å°±ä½ã€‚

3
00:00:07,000 --> 00:00:09,000
é˜Ÿé•¿: æˆ‘ä»¬å‡†å¤‡å¼€å§‹äº†ï¼

4
00:00:10,000 --> 00:00:12,000
[æ—ç™½] è¿™æ˜¯ä¸€ä¸ªå…³äºå†›é˜Ÿè®­ç»ƒçš„æ•…äº‹ã€‚"""
    
    # åˆ›å»ºAgent
    agent = create_subtitle_translation_agent()
    
    # æ‰§è¡Œç¿»è¯‘
    result = agent.translate_subtitle_file(
        srt_content=srt_content,
        target_language="en",
        additional_context="è¿™æ˜¯ä¸€éƒ¨å†›äº‹é¢˜æçš„ç°ä»£å‰§ï¼Œè®²è¿°æµ·å†›è®­ç»ƒçš„æ•…äº‹"
    )
    
    if result["success"]:
        print("âœ… ç¿»è¯‘æˆåŠŸå®Œæˆï¼")
        print("\nğŸ“Š ç¿»è¯‘ç»Ÿè®¡:")
        
        # è§£æç»“æœå¹¶æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        try:
            parse_data = json.loads(result["parse_result"])
            if parse_data["success"]:
                stats = parse_data["data"]["statistics"]
                print(f"  - æ€»æ¡ç›®æ•°: {stats['total_entries']}")
                print(f"  - æ£€æµ‹åˆ°è¯´è¯äºº: {stats['speakers_detected']}")
                print(f"  - æ€»æ—¶é•¿: {stats['total_duration_ms']/1000:.1f}ç§’")
        except:
            print("  - ç»Ÿè®¡ä¿¡æ¯è§£æå¤±è´¥")
        
        print("\nğŸ“ å¯¼å‡ºçš„SRTæ–‡ä»¶é¢„è§ˆ:")
        try:
            export_data = json.loads(result["exported_srt"])
            if export_data["success"]:
                srt_preview = export_data["data"]["srt_content"][:500]
                print(srt_preview + "..." if len(srt_preview) == 500 else srt_preview)
        except:
            print("  - SRTé¢„è§ˆå¤±è´¥")
            
    else:
        print(f"âŒ ç¿»è¯‘å¤±è´¥: {result['error']}")
    
    print("\n" + "=" * 50 + "\n")

def example_2_step_by_step():
    """ç¤ºä¾‹2: åˆ†æ­¥éª¤ä½¿ç”¨å·¥å…·å‡½æ•°"""
    print("ğŸ”§ ç¤ºä¾‹2: åˆ†æ­¥éª¤ä½¿ç”¨å·¥å…·å‡½æ•°")
    print("=" * 50)
    
    # ç¤ºä¾‹SRTå†…å®¹
    srt_content = """1
00:00:01,000 --> 00:00:03,000
å°æ˜: å“¥ï¼Œä½ è§‰å¾—è¿™ä¸ªé¸¡å¨ƒç°è±¡æ€ä¹ˆæ ·ï¼Ÿ

2
00:00:04,000 --> 00:00:06,000
å°å: ç°åœ¨å†…å·å¤ªä¸¥é‡äº†ï¼Œå¤§å®¶éƒ½åœ¨èººå¹³ã€‚

3
00:00:07,000 --> 00:00:09,000
å°æ˜: æ˜¯å•Šï¼Œå‹åŠ›å¤ªå¤§äº†ã€‚"""
    
    print("æ­¥éª¤1: è§£æSRTæ–‡ä»¶")
    parse_result = parse_srt_file(srt_content, detect_speakers=True)
    parse_data = json.loads(parse_result)
    
    if parse_data["success"]:
        print(f"âœ… è§£ææˆåŠŸï¼Œå…± {parse_data['data']['statistics']['total_entries']} ä¸ªæ¡ç›®")
        entries = parse_data["data"]["entries"]
        for entry in entries[:2]:  # æ˜¾ç¤ºå‰2ä¸ªæ¡ç›®
            print(f"  - æ¡ç›®{entry['sequence']}: {entry['speaker']} - {entry['original_text']}")
    else:
        print(f"âŒ è§£æå¤±è´¥: {parse_data['error']}")
        return
    
    print("\næ­¥éª¤2: åˆ†ææ•…äº‹ä¸Šä¸‹æ–‡")
    context_result = analyze_story_context(
        entries=parse_result,
        additional_context='{"title": "ç°ä»£éƒ½å¸‚å‰§", "genre": "modern_drama"}',
        analysis_depth="deep"
    )
    context_data = json.loads(context_result)
    
    if context_data["success"]:
        print("âœ… ä¸Šä¸‹æ–‡åˆ†ææˆåŠŸ")
        context = context_data["data"]["context"]
        print(f"  - å‰§é›†ç±»å‹: {context.get('genre', 'æœªçŸ¥')}")
        print(f"  - è¯­è°ƒé£æ ¼: {context.get('tone_style', 'æœªçŸ¥')}")
        print(f"  - ä¸»è¦è§’è‰²: {len(context.get('characters', []))}")
    else:
        print(f"âŒ ä¸Šä¸‹æ–‡åˆ†æå¤±è´¥: {context_data['error']}")
        return
    
    print("\næ­¥éª¤3: æ‰§è¡Œç¿»è¯‘ (æ—¥è¯­)")
    translation_config = {
        "quality_level": "high",
        "cultural_adaptation": True,
        "maintain_speaker_style": True
    }
    
    translate_result = translate_with_context(
        entries=parse_result,
        target_language="ja",
        story_context=context_result,
        translation_config=json.dumps(translation_config)
    )
    translate_data = json.loads(translate_result)
    
    if translate_data["success"]:
        print("âœ… ç¿»è¯‘æˆåŠŸ")
        summary = translate_data["data"]["translation_summary"]
        print(f"  - ç¿»è¯‘æ¡ç›®æ•°: {summary['total_entries']}")
        print(f"  - å¹³å‡ç½®ä¿¡åº¦: {summary['average_confidence']:.2f}")
        print(f"  - ç¿»è¯‘ç­–ç•¥: {summary['translation_strategy']}")
    else:
        print(f"âŒ ç¿»è¯‘å¤±è´¥: {translate_data['error']}")
        return
    
    print("\næ­¥éª¤4: è´¨é‡éªŒè¯")
    quality_result = validate_translation_quality(
        original_entries=parse_result,
        translated_entries=translate_result,
        target_language="ja"
    )
    quality_data = json.loads(quality_result)
    
    if quality_data["success"]:
        print("âœ… è´¨é‡éªŒè¯å®Œæˆ")
        metrics = quality_data["data"]["quality_metrics"]
        print(f"  - æ€»ä½“è¯„åˆ†: {metrics['overall_score']:.2f}")
        print(f"  - å‡†ç¡®æ€§: {metrics['accuracy_score']:.2f}")
        print(f"  - æµç•…æ€§: {metrics['fluency_score']:.2f}")
        print(f"  - ä¸€è‡´æ€§: {metrics['consistency_score']:.2f}")
        print(f"  - å‘ç°é—®é¢˜: {quality_data['data']['validation_summary']['issues_found']} ä¸ª")
    else:
        print(f"âŒ è´¨é‡éªŒè¯å¤±è´¥: {quality_data['error']}")
        return
    
    print("\næ­¥éª¤5: å¯¼å‡ºSRTæ–‡ä»¶")
    export_config = {
        "include_speaker_names": True,
        "speaker_name_format": "{speaker}: {text}",
        "add_metadata": True
    }
    
    export_result = export_translated_srt(
        translated_entries=translate_result,
        export_config=json.dumps(export_config)
    )
    export_data = json.loads(export_result)
    
    if export_data["success"]:
        print("âœ… SRTå¯¼å‡ºæˆåŠŸ")
        info = export_data["data"]["export_info"]
        print(f"  - å»ºè®®æ–‡ä»¶å: {info['suggested_filename']}")
        print(f"  - æ–‡ä»¶å¤§å°: {info['file_size_bytes']} å­—èŠ‚")
        print(f"  - ç¼–ç : {info['encoding']}")
        
        print("\nğŸ“ å¯¼å‡ºå†…å®¹é¢„è§ˆ:")
        srt_content = export_data["data"]["srt_content"]
        lines = srt_content.split('\n')[:15]  # æ˜¾ç¤ºå‰15è¡Œ
        print('\n'.join(lines))
        if len(srt_content.split('\n')) > 15:
            print("...")
    else:
        print(f"âŒ SRTå¯¼å‡ºå¤±è´¥: {export_data['error']}")
    
    print("\n" + "=" * 50 + "\n")

def example_3_multiple_languages():
    """ç¤ºä¾‹3: å¤šè¯­è¨€ç¿»è¯‘å¯¹æ¯”"""
    print("ğŸŒ ç¤ºä¾‹3: å¤šè¯­è¨€ç¿»è¯‘å¯¹æ¯”")
    print("=" * 50)
    
    # ç®€å•çš„SRTå†…å®¹
    srt_content = """1
00:00:01,000 --> 00:00:03,000
è€æ¿: ä½ å¥½ï¼Œæ¬¢è¿æ¥åˆ°æˆ‘ä»¬å…¬å¸ï¼

2
00:00:04,000 --> 00:00:06,000
å‘˜å·¥: è°¢è°¢è€æ¿ï¼Œæˆ‘ä¼šåŠªåŠ›å·¥ä½œçš„ã€‚"""
    
    # ç›®æ ‡è¯­è¨€åˆ—è¡¨
    target_languages = ["en", "ja", "ko", "es"]
    language_names = {
        "en": "è‹±è¯­",
        "ja": "æ—¥è¯­", 
        "ko": "éŸ©è¯­",
        "es": "è¥¿ç­ç‰™è¯­"
    }
    
    # è§£æSRT
    parse_result = parse_srt_file(srt_content, detect_speakers=True)
    parse_data = json.loads(parse_result)
    
    if not parse_data["success"]:
        print(f"âŒ SRTè§£æå¤±è´¥: {parse_data['error']}")
        return
    
    # åˆ†æä¸Šä¸‹æ–‡
    context_result = analyze_story_context(
        entries=parse_result,
        additional_context='{"title": "èŒåœºå‰§", "genre": "workplace"}',
        analysis_depth="standard"
    )
    
    print("ğŸ”„ å¼€å§‹å¤šè¯­è¨€ç¿»è¯‘...")
    
    for lang_code in target_languages:
        lang_name = language_names[lang_code]
        print(f"\nğŸ“ ç¿»è¯‘åˆ°{lang_name} ({lang_code}):")
        
        # æ‰§è¡Œç¿»è¯‘
        translate_result = translate_with_context(
            entries=parse_result,
            target_language=lang_code,
            story_context=context_result,
            translation_config='{"quality_level": "high"}'
        )
        translate_data = json.loads(translate_result)
        
        if translate_data["success"]:
            # æ˜¾ç¤ºç¿»è¯‘ç»“æœ
            entries = translate_data["data"]["translated_entries"]
            for entry in entries:
                speaker = entry.get("speaker", "")
                original = entry.get("original_text", "")
                translated = entry.get("translated_text", "")
                print(f"  åŸæ–‡: {speaker}: {original}")
                print(f"  è¯‘æ–‡: {speaker}: {translated}")
                print()
        else:
            print(f"  âŒ ç¿»è¯‘å¤±è´¥: {translate_data['error']}")
    
    print("=" * 50 + "\n")

def example_4_quality_analysis():
    """ç¤ºä¾‹4: ç¿»è¯‘è´¨é‡åˆ†æ"""
    print("ğŸ“Š ç¤ºä¾‹4: ç¿»è¯‘è´¨é‡åˆ†æ")
    print("=" * 50)
    
    # åˆ›å»ºä¸€ä¸ªæœ‰é—®é¢˜çš„ç¿»è¯‘ç¤ºä¾‹
    original_entries = [
        {
            "sequence": 1,
            "start_time": "00:00:01,000",
            "end_time": "00:00:03,000",
            "original_text": "ä½ å¥½ï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼",
            "speaker": "å°æ˜"
        },
        {
            "sequence": 2,
            "start_time": "00:00:04,000",
            "end_time": "00:00:06,000",
            "original_text": "æˆ‘ä¹Ÿå¾ˆé«˜å…´è§åˆ°ä½ ã€‚",
            "speaker": "å°å"
        }
    ]
    
    # åˆ›å»ºæœ‰é—®é¢˜çš„ç¿»è¯‘
    problematic_translation = [
        {
            "sequence": 1,
            "start_time": "00:00:01,000",  # æ—¶é—´ç ä¸€è‡´
            "end_time": "00:00:03,000",
            "original_text": "ä½ å¥½ï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼",
            "translated_text": "[å¾…ç¿»è¯‘] Hello, nice to meet you! This is a very long translation that exceeds the recommended character limit for subtitles",
            "speaker": "å°æ˜"
        },
        {
            "sequence": 2,
            "start_time": "00:00:00,000",  # æ—¶é—´ç ä¸ä¸€è‡´
            "end_time": "00:00:06,000",
            "original_text": "æˆ‘ä¹Ÿå¾ˆé«˜å…´è§åˆ°ä½ ã€‚",
            "translated_text": "Nice to meet you too",  # ç¼ºå°‘é—®å·
            "speaker": "å°å"
        }
    ]
    
    # æ‰§è¡Œè´¨é‡éªŒè¯
    quality_result = validate_translation_quality(
        original_entries=json.dumps(original_entries),
        translated_entries=json.dumps(problematic_translation),
        target_language="en",
        validation_config='{"detailed_analysis": true}'
    )
    
    quality_data = json.loads(quality_result)
    
    if quality_data["success"]:
        print("âœ… è´¨é‡åˆ†æå®Œæˆ")
        
        # æ˜¾ç¤ºè´¨é‡æŒ‡æ ‡
        metrics = quality_data["data"]["quality_metrics"]
        print(f"\nğŸ“ˆ è´¨é‡æŒ‡æ ‡:")
        print(f"  - æ€»ä½“è¯„åˆ†: {metrics['overall_score']:.2f}")
        print(f"  - å‡†ç¡®æ€§: {metrics['accuracy_score']:.2f}")
        print(f"  - æµç•…æ€§: {metrics['fluency_score']:.2f}")
        print(f"  - ä¸€è‡´æ€§: {metrics['consistency_score']:.2f}")
        print(f"  - æ–‡åŒ–é€‚é…: {metrics['cultural_adaptation_score']:.2f}")
        print(f"  - æ—¶é—´æ§åˆ¶: {metrics['timing_score']:.2f}")
        
        # æ˜¾ç¤ºå‘ç°çš„é—®é¢˜
        issues = quality_data["data"]["detailed_issues"]
        print(f"\nğŸ” å‘ç°çš„é—®é¢˜ ({len(issues)} ä¸ª):")
        for i, issue in enumerate(issues, 1):
            print(f"  {i}. æ¡ç›®{issue['sequence']} - {issue['issue_type']}")
            print(f"     æè¿°: {issue['description']}")
            print(f"     ä¸¥é‡ç¨‹åº¦: {issue['severity']}")
            print(f"     å»ºè®®: {issue['suggestion']}")
            print()
        
        # æ˜¾ç¤ºæ”¹è¿›å»ºè®®
        recommendations = metrics.get("recommendations", [])
        if recommendations:
            print("ğŸ’¡ æ”¹è¿›å»ºè®®:")
            for i, rec in enumerate(recommendations, 1):
                print(f"  {i}. {rec}")
        
        # æ˜¾ç¤ºé—®é¢˜ç»Ÿè®¡
        issue_stats = quality_data["data"]["issue_statistics"]
        print(f"\nğŸ“Š é—®é¢˜ç»Ÿè®¡:")
        for issue_type, stats in issue_stats.items():
            print(f"  - {issue_type}: {stats['total']} ä¸ª (é«˜:{stats['high']}, ä¸­:{stats['medium']}, ä½:{stats['low']})")
        
    else:
        print(f"âŒ è´¨é‡åˆ†æå¤±è´¥: {quality_data['error']}")
    
    print("\n" + "=" * 50 + "\n")

def main():
    """ä¸»å‡½æ•°ï¼Œè¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("ğŸ¬ Strands Agentå­—å¹•ç¿»è¯‘ç³»ç»Ÿä½¿ç”¨ç¤ºä¾‹")
    print("=" * 60)
    print("æœ¬ç¤ºä¾‹å±•ç¤ºäº†å®Œæ•´çš„å­—å¹•ç¿»è¯‘å·¥ä½œæµç¨‹")
    print("åŒ…æ‹¬è§£æã€åˆ†æã€ç¿»è¯‘ã€éªŒè¯å’Œå¯¼å‡ºç­‰æ­¥éª¤")
    print("=" * 60)
    print()
    
    try:
        # è¿è¡Œæ‰€æœ‰ç¤ºä¾‹
        example_1_basic_translation()
        example_2_step_by_step()
        example_3_multiple_languages()
        example_4_quality_analysis()
        
        print("ğŸ‰ æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
        print("\nğŸ“š æ›´å¤šä¿¡æ¯è¯·å‚è€ƒ:")
        print("  - README.md: è¯¦ç»†æ–‡æ¡£")
        print("  - test_enhanced_tools.py: æµ‹è¯•ç”¨ä¾‹")
        print("  - subtitle_translation_agent.py: Agentå®ç°")
        
    except Exception as e:
        print(f"âŒ ç¤ºä¾‹è¿è¡Œå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()